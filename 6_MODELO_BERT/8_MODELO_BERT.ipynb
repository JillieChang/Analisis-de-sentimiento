{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) es un nuevo modelo de representación de lenguaje diseñado para entrenar previamente representaciones bidireccionales profundas a partir de un texto sin etiquetar mediante el condicionamiento conjunto del contexto izquierdo y derecho en todas las capas. Como resultado, el modelo BERT previamente entrenado se puede ajustar con solo una capa de salida adicional para crear modelos aplicados a una amplia gama de tareas, como respuesta a preguntas e inferencia de lenguaje, clasificación de textos, entre otras.\n",
    "\n",
    "Para mayor información, visitar los siguientes enlaces:\n",
    "\n",
    "- https://github.com/google-research/bert\n",
    "- https://huggingface.co/bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código es una adaptación de Koksal, A. (2020), disponible en:\n",
    "\n",
    "- https://github.com/akoksal/BERT-Sentiment-Analysis-Turkish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Instalar paquetes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Instalar Pytorch\n",
    "\n",
    "Es una libreria basado en Python, diseñado para realizar cálculos numéricos haciendo uso de la programación de tensores. Además permite su ejecución en GPU para acelerar los cálculos. \n",
    "\n",
    "PyTorch dispone una interfaz muy sencilla para la creación de redes neuronales pese a trabajar de forma directa con tensores sin la necesidad de una librería a un nivel superior como pueda ser Keras para Theano o Tensorflow.\n",
    "\n",
    "PyTorch dispone de soporte para su ejecución en tarjetas gráficas (GPU), utiliza internamente CUDA, una API que conecta la CPU con la GPU que ha sido desarrollado por NVIDIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\jilli\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    pytorch-1.7.0              |py3.8_cuda110_cudnn8_0      1003.4 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:      1003.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  torchvision        pytorch/win-64::torchvision-0.8.1-py38_cu110\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  pytorch                                           PyTorch --> pytorch\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   0% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB |            |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   1% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 1          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   2% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 2          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   3% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 3          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   4% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 4          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   5% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 5          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   6% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 6          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   7% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 7          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% \n",
      "pytorch-1.7.0        | 1003.4 MB | 8          |   8% "
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que el procesamiento sea más rápido se puede utilizar GPU con el modelo CUDA (Compute Unified Device Architecture) que es una plataforma de computación en paralelo que incluye un compilador y un conjunto de herramientas de desarrollo creadas por nVidia. Sin embargo, debido a que esta computadora no cuenta con GPU, se debe trabajar solo con CPU\n",
    "\n",
    "Para mayor info sobre \"Working with GPU packages\" visitar:\n",
    "\n",
    "- https://docs.anaconda.com/anaconda/user-guide/tasks/gpu-packages/\n",
    "\n",
    "A continuación, se verifica que la computadora cuenta solo con CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# ver si se esta utilizando gpu o cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch._C._cuda_getCompiledVersion())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contar cuantos gpus tiene computadora para procesamiento paralelo\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Instalar Tensorflow\n",
    "\n",
    "TensorFlow es una biblioteca de aprendizaje automático general, pero más popular para aplicaciones de aprendizaje profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached tensorflow-2.3.1-cp38-cp38-win_amd64.whl (342.5 MB)\n",
      "Processing c:\\users\\jilli\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.13.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.33.2-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: termcolor, keras-preprocessing, protobuf, grpcio, astunparse, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, google-auth-oauthlib, tensorboard-plugin-wit, markdown, tensorboard, opt-einsum, tensorflow-estimator, google-pasta, gast, tensorflow\n",
      "Successfully installed astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 keras-preprocessing-1.1.2 markdown-3.3.3 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in c:\\users\\jilli\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.3.3 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Instalar Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras in c:\\users\\jilli\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from Keras) (1.5.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from Keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from Keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from Keras) (1.18.5)\n",
      "Requirement already satisfied: six in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from h5py->Keras) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.4 Instalar transformers\n",
    "\n",
    "Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provee de arquitecturas de propósitos generales (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) para el entendimiento del lenguaje natural con más de 32+ modelos pre entrenados en más de 100 lenguajes (interoperatividad profunda entre TensorFlow 2.0 y PyTorch)\n",
    "\n",
    "- https://huggingface.co/transformers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.94-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from transformers) (3.13.0)\n",
      "Collecting tokenizers==0.9.2\n",
      "  Downloading tokenizers-0.9.2-cp38-cp38-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jilli\\anaconda3\\lib\\site-packages (from protobuf->transformers) (49.2.0.post20200714)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893262 sha256=14807fba660192285eef4740e267d25445c93ca52eadd9801ac58e070ba07163\n",
      "  Stored in directory: c:\\users\\jilli\\appdata\\local\\pip\\cache\\wheels\\7b\\78\\f4\\27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re as re\n",
    "import emoji as emoji\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Importar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Tweet_User_Id</th>\n",
       "      <th>Tweet_User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Replies</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Pais</th>\n",
       "      <th>Tweet_Source</th>\n",
       "      <th>lang</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text2</th>\n",
       "      <th>Ind_emoji</th>\n",
       "      <th>clean_text3</th>\n",
       "      <th>Ind_Posi</th>\n",
       "      <th>Ind_Nega</th>\n",
       "      <th>Target</th>\n",
       "      <th>text_limpio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243708742161633280</td>\n",
       "      <td>1113483741765222401</td>\n",
       "      <td>MJosealf</td>\n",
       "      <td>Pero que falta de criterio!!!! Y esto está pas...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-28 01:17:25+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chi</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>es</td>\n",
       "      <td>{'🤬': 'face with symbols on mouth'}</td>\n",
       "      <td>Pero que falta de criterio!!!! Y esto está pas...</td>\n",
       "      <td>1</td>\n",
       "      <td>['face with symbols on mouth']</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>pero que falta de criterio y esto está pasando...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet_Id        Tweet_User_Id Tweet_User  \\\n",
       "1  1243708742161633280  1113483741765222401   MJosealf   \n",
       "\n",
       "                                                Text  Retweets  Favorites  \\\n",
       "1  Pero que falta de criterio!!!! Y esto está pas...         0          0   \n",
       "\n",
       "   Replies                   Datetime hashtags Pais        Tweet_Source lang  \\\n",
       "1        0  2020-03-28 01:17:25+00:00      NaN  Chi  Twitter for iPhone   es   \n",
       "\n",
       "                            clean_text  \\\n",
       "1  {'🤬': 'face with symbols on mouth'}   \n",
       "\n",
       "                                         clean_text2  Ind_emoji  \\\n",
       "1  Pero que falta de criterio!!!! Y esto está pas...          1   \n",
       "\n",
       "                      clean_text3  Ind_Posi  Ind_Nega Target  \\\n",
       "1  ['face with symbols on mouth']         0         1      N   \n",
       "\n",
       "                                         text_limpio  \n",
       "1  pero que falta de criterio y esto está pasando...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_csv(r'C:\\Users\\jilli\\Desktop\\TFM\\2\\train.csv')\n",
    "train[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Pais</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1249382391120048129</td>\n",
       "      <td>En esta cuarentena, mi psicólogo me ha dicho q...</td>\n",
       "      <td>2020-04-12 17:02:29+00:00</td>\n",
       "      <td>Per</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243708742161633280</td>\n",
       "      <td>Pero que falta de criterio!!!! Y esto está pas...</td>\n",
       "      <td>2020-03-28 01:17:25+00:00</td>\n",
       "      <td>Chi</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1239277368986009600</td>\n",
       "      <td>Gracias Lore, acá todavía no es tan severa la ...</td>\n",
       "      <td>2020-03-15 19:48:44+00:00</td>\n",
       "      <td>Arg</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1235980525694783488</td>\n",
       "      <td>🇵🇪 Casos confirmados: #Dengue: 8221 (18 ☠️) Se...</td>\n",
       "      <td>2020-03-06 17:28:15+00:00</td>\n",
       "      <td>Per</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242148777357705216</td>\n",
       "      <td>Yo si es que digo que ustedes aunque no conozc...</td>\n",
       "      <td>2020-03-23 17:58:41+00:00</td>\n",
       "      <td>Col</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22269</th>\n",
       "      <td>1285048010687291392</td>\n",
       "      <td>La pandemia me ha quitado tanto momentos lindo...</td>\n",
       "      <td>2020-07-20 03:04:55+00:00</td>\n",
       "      <td>Col</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22270</th>\n",
       "      <td>1249688695256403969</td>\n",
       "      <td>GRÁBATE MIENTRAS VUELAS, SIEMPRE Y CUANDO NO E...</td>\n",
       "      <td>2020-04-13 13:19:37+00:00</td>\n",
       "      <td>Chi</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22271</th>\n",
       "      <td>1251896052279582721</td>\n",
       "      <td>Lo bueno de la cuarentena es que ya no hay que...</td>\n",
       "      <td>2020-04-19 15:30:52+00:00</td>\n",
       "      <td>Arg</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22272</th>\n",
       "      <td>1276917451557568515</td>\n",
       "      <td>Día 104 de cuarentena: #BuenSabado y muchas gr...</td>\n",
       "      <td>2020-06-27 16:36:59+00:00</td>\n",
       "      <td>Arg</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22273</th>\n",
       "      <td>1261733492435623937</td>\n",
       "      <td>FELIZ CUMPLEAÑOS LUCIÓ, QUE DIOS TE BENDIGA, Q...</td>\n",
       "      <td>2020-05-16 19:01:21+00:00</td>\n",
       "      <td>Arg</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22274 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tweet_Id                                               Text  \\\n",
       "0      1249382391120048129  En esta cuarentena, mi psicólogo me ha dicho q...   \n",
       "1      1243708742161633280  Pero que falta de criterio!!!! Y esto está pas...   \n",
       "2      1239277368986009600  Gracias Lore, acá todavía no es tan severa la ...   \n",
       "3      1235980525694783488  🇵🇪 Casos confirmados: #Dengue: 8221 (18 ☠️) Se...   \n",
       "4      1242148777357705216  Yo si es que digo que ustedes aunque no conozc...   \n",
       "...                    ...                                                ...   \n",
       "22269  1285048010687291392  La pandemia me ha quitado tanto momentos lindo...   \n",
       "22270  1249688695256403969  GRÁBATE MIENTRAS VUELAS, SIEMPRE Y CUANDO NO E...   \n",
       "22271  1251896052279582721  Lo bueno de la cuarentena es que ya no hay que...   \n",
       "22272  1276917451557568515  Día 104 de cuarentena: #BuenSabado y muchas gr...   \n",
       "22273  1261733492435623937  FELIZ CUMPLEAÑOS LUCIÓ, QUE DIOS TE BENDIGA, Q...   \n",
       "\n",
       "                        Datetime Pais Target  \n",
       "0      2020-04-12 17:02:29+00:00  Per      N  \n",
       "1      2020-03-28 01:17:25+00:00  Chi      N  \n",
       "2      2020-03-15 19:48:44+00:00  Arg      P  \n",
       "3      2020-03-06 17:28:15+00:00  Per      N  \n",
       "4      2020-03-23 17:58:41+00:00  Col      P  \n",
       "...                          ...  ...    ...  \n",
       "22269  2020-07-20 03:04:55+00:00  Col      P  \n",
       "22270  2020-04-13 13:19:37+00:00  Chi      P  \n",
       "22271  2020-04-19 15:30:52+00:00  Arg      P  \n",
       "22272  2020-06-27 16:36:59+00:00  Arg      P  \n",
       "22273  2020-05-16 19:01:21+00:00  Arg      P  \n",
       "\n",
       "[22274 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train  = DataFrame(train,columns=[\"Tweet_Id\", \"Text\",\"Datetime\", \"Pais\",\"Target\"])\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir funciones de limpieza\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+','link',text) #normURLs\n",
    "    text = re.sub(r'www.\\S+','link',text) #normWWW\n",
    "    text = re.sub(emoji.get_emoji_regexp(), r\"\", text)   #strip_emoji\n",
    "    text = re.sub(r'@\\S+','usuario',text) #norm_user\n",
    "    text = re.sub(r'#\\S+','hashtag',text) #norm_hashtags    \n",
    "    text = re.sub(r'– at.+$','',text) #remove_at\n",
    "    text = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', 'risas', text, flags=re.IGNORECASE)#normalize_risas\n",
    "    text = re.sub(r'\\b(juas+|lol+)\\b', 'risas', text, flags=re.IGNORECASE)#normalize_risas\n",
    "    text= re.sub(r'\\b(ja|jaa)\\b', 'risas', text, flags=re.IGNORECASE)#normalize_risas\n",
    "    text = re.sub(r'[^\\w\\s]','',text) #punct_re_regex\n",
    "    text= re.sub(r'(.)\\1{2,}', r'\\1\\1', text, flags=re.IGNORECASE)#normalize_repet #quita signos punt   \n",
    "    text = re.sub('\\d+', '', text) #remove_digits\n",
    "    text = text.replace(\"_\", \"\")     #quita _\n",
    "    text = re.sub(r' +',' ',text) #remove_spaces\n",
    "    text = text.strip()     \n",
    "    return text\n",
    "\n",
    "#jergas\n",
    "jerga = [('d','de'), ('[qk]','que'), ('xo','pero'),('fav','favorito'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
    "              ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),('ud','usted'),\n",
    "             ('uds','ustedes'),('(ctm|alv|hdp)','insulto'), ('sr','señor'),('(fds|finde)','fin de semana'),('app','aplicación'),\n",
    "         ('(La concha de tu madre|conchasumadres|reconchadesumadre|conchatumadre|la concha de la madre|la concha de su madre|La concha bien de su madre|la concha de tu hermana)','insulto'),\n",
    "              ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'),('bb','bebé'), ('x','por'), ('\\+','mas')]\n",
    "def normalize_jergas(message):\n",
    "    for s,t in jerga:\n",
    "        message = re.sub(r'\\b{0}\\b'.format(s), t, message, flags=re.IGNORECASE)\n",
    "    return message   \n",
    "\n",
    "#vocales\n",
    "vocales= [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]\n",
    "def norma_vocales(message):\n",
    "    for s,t in vocales:\n",
    "        message = re.sub(r'{0}'.format(s), t, message, flags=re.IGNORECASE)\n",
    "    return message\n",
    "\n",
    "# Stemming\n",
    "_stemmer = SnowballStemmer('spanish')\n",
    "_tokenizer = TweetTokenizer().tokenize\n",
    "    \n",
    "def stem(message):\n",
    "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Pais</th>\n",
       "      <th>Target</th>\n",
       "      <th>text_limpio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1249382391120048129</td>\n",
       "      <td>En esta cuarentena, mi psicólogo me ha dicho q...</td>\n",
       "      <td>2020-04-12 17:02:29+00:00</td>\n",
       "      <td>Per</td>\n",
       "      <td>N</td>\n",
       "      <td>en esta cuarenten mi psicolog me ha dich que l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243708742161633280</td>\n",
       "      <td>Pero que falta de criterio!!!! Y esto está pas...</td>\n",
       "      <td>2020-03-28 01:17:25+00:00</td>\n",
       "      <td>Chi</td>\n",
       "      <td>N</td>\n",
       "      <td>per que falt de criteri y esto esta pas en muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1239277368986009600</td>\n",
       "      <td>Gracias Lore, acá todavía no es tan severa la ...</td>\n",
       "      <td>2020-03-15 19:48:44+00:00</td>\n",
       "      <td>Arg</td>\n",
       "      <td>P</td>\n",
       "      <td>graci lor aca todavi no es tan sever la cuaren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1235980525694783488</td>\n",
       "      <td>🇵🇪 Casos confirmados: #Dengue: 8221 (18 ☠️) Se...</td>\n",
       "      <td>2020-03-06 17:28:15+00:00</td>\n",
       "      <td>Per</td>\n",
       "      <td>N</td>\n",
       "      <td>cas confirm hashtag selv hashtag lim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242148777357705216</td>\n",
       "      <td>Yo si es que digo que ustedes aunque no conozc...</td>\n",
       "      <td>2020-03-23 17:58:41+00:00</td>\n",
       "      <td>Col</td>\n",
       "      <td>P</td>\n",
       "      <td>yo si es que dig que usted aunqu no conozc a l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet_Id                                               Text  \\\n",
       "0  1249382391120048129  En esta cuarentena, mi psicólogo me ha dicho q...   \n",
       "1  1243708742161633280  Pero que falta de criterio!!!! Y esto está pas...   \n",
       "2  1239277368986009600  Gracias Lore, acá todavía no es tan severa la ...   \n",
       "3  1235980525694783488  🇵🇪 Casos confirmados: #Dengue: 8221 (18 ☠️) Se...   \n",
       "4  1242148777357705216  Yo si es que digo que ustedes aunque no conozc...   \n",
       "\n",
       "                    Datetime Pais Target  \\\n",
       "0  2020-04-12 17:02:29+00:00  Per      N   \n",
       "1  2020-03-28 01:17:25+00:00  Chi      N   \n",
       "2  2020-03-15 19:48:44+00:00  Arg      P   \n",
       "3  2020-03-06 17:28:15+00:00  Per      N   \n",
       "4  2020-03-23 17:58:41+00:00  Col      P   \n",
       "\n",
       "                                         text_limpio  \n",
       "0  en esta cuarenten mi psicolog me ha dich que l...  \n",
       "1  per que falt de criteri y esto esta pas en muc...  \n",
       "2  graci lor aca todavi no es tan sever la cuaren...  \n",
       "3               cas confirm hashtag selv hashtag lim  \n",
       "4  yo si es que dig que usted aunqu no conozc a l...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aplicar preprocesos\n",
    "train[\"text_limpio\"]=train[\"Text\"].to_numpy() # duplica columna Text con formato numpy\n",
    "train[\"text_limpio\"]=train[\"text_limpio\"].apply(lambda tweet: normalize_jergas(tweet)) #normaliza jergas\n",
    "train[\"text_limpio\"]=train[\"text_limpio\"].apply(lambda tweet: norma_vocales(tweet)) #normaliza tildes\n",
    "train[\"text_limpio\"]=train[\"text_limpio\"].apply(lambda tweet: clean(tweet)) # limpieza\n",
    "train[\"text_limpio\"]=train[\"text_limpio\"].apply(lambda tweet: stem(tweet)) #stemming\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>text_limpio</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1249382391120048129</td>\n",
       "      <td>en esta cuarenten mi psicolog me ha dich que l...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243708742161633280</td>\n",
       "      <td>per que falt de criteri y esto esta pas en muc...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1239277368986009600</td>\n",
       "      <td>graci lor aca todavi no es tan sever la cuaren...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1235980525694783488</td>\n",
       "      <td>cas confirm hashtag selv hashtag lim</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242148777357705216</td>\n",
       "      <td>yo si es que dig que usted aunqu no conozc a l...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22269</th>\n",
       "      <td>1285048010687291392</td>\n",
       "      <td>la pandemi me ha quit tant moment lind per has...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22270</th>\n",
       "      <td>1249688695256403969</td>\n",
       "      <td>grabat mientr vuel siempr y cuand no estes sol...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22271</th>\n",
       "      <td>1251896052279582721</td>\n",
       "      <td>lo buen de la cuarenten es que ya no hay que c...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22272</th>\n",
       "      <td>1276917451557568515</td>\n",
       "      <td>dia de cuarenten hashtag y much graci a usuari...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22273</th>\n",
       "      <td>1261733492435623937</td>\n",
       "      <td>feliz cumpleañ luci que dios te bendig que des...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22274 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tweet_Id                                        text_limpio  \\\n",
       "0      1249382391120048129  en esta cuarenten mi psicolog me ha dich que l...   \n",
       "1      1243708742161633280  per que falt de criteri y esto esta pas en muc...   \n",
       "2      1239277368986009600  graci lor aca todavi no es tan sever la cuaren...   \n",
       "3      1235980525694783488               cas confirm hashtag selv hashtag lim   \n",
       "4      1242148777357705216  yo si es que dig que usted aunqu no conozc a l...   \n",
       "...                    ...                                                ...   \n",
       "22269  1285048010687291392  la pandemi me ha quit tant moment lind per has...   \n",
       "22270  1249688695256403969  grabat mientr vuel siempr y cuand no estes sol...   \n",
       "22271  1251896052279582721  lo buen de la cuarenten es que ya no hay que c...   \n",
       "22272  1276917451557568515  dia de cuarenten hashtag y much graci a usuari...   \n",
       "22273  1261733492435623937  feliz cumpleañ luci que dios te bendig que des...   \n",
       "\n",
       "      Target  \n",
       "0          N  \n",
       "1          N  \n",
       "2          P  \n",
       "3          N  \n",
       "4          P  \n",
       "...      ...  \n",
       "22269      P  \n",
       "22270      P  \n",
       "22271      P  \n",
       "22272      P  \n",
       "22273      P  \n",
       "\n",
       "[22274 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se extraen campos a utilizar\n",
    "df_BERT = DataFrame(train, columns= ['Tweet_Id','text_limpio','Target'])\n",
    "df_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se renombran los campos\n",
    "df_BERT = df_BERT.rename(columns={'Tweet_Id':'id','text_limpio':'sentence','Target':'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1249382391120048129</td>\n",
       "      <td>en esta cuarenten mi psicolog me ha dich que l...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                           sentence  \\\n",
       "0  1249382391120048129  en esta cuarenten mi psicolog me ha dich que l...   \n",
       "\n",
       "  value  \n",
       "0     N  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BERT.reset_index(drop=True,inplace=True)\n",
    "df_BERT[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte archivo a formato json\n",
    "df_BERT.to_json(r'C:\\Users\\jilli\\Desktop\\TFM\\2\\BERT_sample\\df_BERT.json',orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define función para colocar filtros adicionales si se requiere realizar una limpieza adicional \n",
    "def filter(text):\n",
    "    final_text = ''\n",
    "    for word in text.split():\n",
    "        if word.startswith('@'):\n",
    "            continue\n",
    "        else:\n",
    "            final_text += word+' '\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Subdividir muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra de entrenamiento y validación\n",
    "train, validation = train_test_split(df_BERT, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json(r'C:/Users/jilli/Desktop/TFM/2/BERT_sample/df_BERT_train.json',orient=\"records\")\n",
    "validation.to_json(r'C:/Users/jilli/Desktop/TFM/2/BERT_sample/df_BERT_validation.json',orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241218096221827072</td>\n",
       "      <td>señoress en tiemp de pandemi nad mejor que pel...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1254624530674393088</td>\n",
       "      <td>per en cuarenten</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1260601329375940610</td>\n",
       "      <td>mi joystickn alcanz a lleg graci minsal por ha...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1268498533906292738</td>\n",
       "      <td>habl de bellez en la cuarenten bogotan hashtag</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1252826196217262081</td>\n",
       "      <td>hashtag hashtag hashtag vam aguant</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>1273759524126588930</td>\n",
       "      <td>dias de cuarenten y vam record tras record</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>1262147333250322432</td>\n",
       "      <td>nos anim al cort de pel muchisim graci por el ...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9543</th>\n",
       "      <td>1252681613776949248</td>\n",
       "      <td>no doy mas mabel quier sal hashtag hashtag has...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9544</th>\n",
       "      <td>1251206243307393026</td>\n",
       "      <td>vam com avion hashtag</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>1246655965375746055</td>\n",
       "      <td>acab de enter que una chic que entren en el bo...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9546 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                           sentence  \\\n",
       "0     1241218096221827072  señoress en tiemp de pandemi nad mejor que pel...   \n",
       "1     1254624530674393088                                   per en cuarenten   \n",
       "2     1260601329375940610  mi joystickn alcanz a lleg graci minsal por ha...   \n",
       "3     1268498533906292738     habl de bellez en la cuarenten bogotan hashtag   \n",
       "4     1252826196217262081                 hashtag hashtag hashtag vam aguant   \n",
       "...                   ...                                                ...   \n",
       "9541  1273759524126588930         dias de cuarenten y vam record tras record   \n",
       "9542  1262147333250322432  nos anim al cort de pel muchisim graci por el ...   \n",
       "9543  1252681613776949248  no doy mas mabel quier sal hashtag hashtag has...   \n",
       "9544  1251206243307393026                              vam com avion hashtag   \n",
       "9545  1246655965375746055  acab de enter que una chic que entren en el bo...   \n",
       "\n",
       "     value  \n",
       "0        P  \n",
       "1        N  \n",
       "2        N  \n",
       "3        P  \n",
       "4        P  \n",
       "...    ...  \n",
       "9541     N  \n",
       "9542     P  \n",
       "9543     N  \n",
       "9544     N  \n",
       "9545     N  \n",
       "\n",
       "[9546 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestra de test\n",
    "base_predict= pd.read_csv(r'C:\\Users\\jilli\\Desktop\\TFM\\2\\test.csv')\n",
    "\n",
    "base_predict[\"text_limpio\"]=base_predict[\"Text\"].to_numpy() # duplica columna Text con formato numpy\n",
    "base_predict[\"text_limpio\"]=base_predict[\"text_limpio\"].apply(lambda tweet: normalize_jergas(tweet)) #normaliza jergas\n",
    "base_predict[\"text_limpio\"]=base_predict[\"text_limpio\"].apply(lambda tweet: norma_vocales(tweet)) #normaliza tildes\n",
    "base_predict[\"text_limpio\"]=base_predict[\"text_limpio\"].apply(lambda tweet: clean(tweet)) \n",
    "base_predict[\"text_limpio\"]=base_predict[\"text_limpio\"].apply(lambda tweet: stem(tweet))\n",
    "\n",
    "base_predict = DataFrame(base_predict, columns= ['Tweet_Id','text_limpio','Target'])\n",
    "base_predict = base_predict.rename(columns={'Tweet_Id':'id','text_limpio':'sentence','Target':'value'})\n",
    "base_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predict.to_json(r'C:/Users/jilli/Desktop/TFM/2/BERT_sample/df_BERT_test.json',orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_json(r'C:/Users/jilli/Desktop/TFM/1_TFM_BERT/train.json',orient=\"records\")\n",
    "#validation.to_json(r'C:/Users/jilli/Desktop/TFM/1_TFM_BERT/validation.json',orient=\"records\")\n",
    "#test.to_json(r'C:/Users/jilli/Desktop/TFM/1_TFM_BERT/test.json',orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecer rutas de bases de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'C:/Users/jilli/Desktop/TFM/2/BERT_sample/df_BERT_train.json'\n",
    "val_path = 'C:/Users/jilli/Desktop/TFM/2/BERT_sample/df_BERT_validation.json'\n",
    "test_path = 'C:/Users/jilli/Desktop/TFM/2/BERT_sample/df_BERT_test.json'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar modelo de transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert = AutoModel.from_pretrained(\"bert-base-multilingual-cased\").to(device) \n",
    "\n",
    "def feature_extraction(text):\n",
    "    x = tokenizer.encode(filter(text))\n",
    "    with torch.no_grad():\n",
    "        x, _ = bert(torch.stack([torch.tensor(x)]).to(device))\n",
    "        return list(x[0][0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, 'r') as f:\n",
    "    train = json.load(f)\n",
    "with open(val_path, 'r') as f:\n",
    "    val = json.load(f)\n",
    "with open(test_path, 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17819/17819 [38:37<00:00,  7.69it/s]\n",
      "100%|██████████| 4455/4455 [11:07<00:00,  6.68it/s]\n",
      "100%|██████████| 9546/9546 [23:35<00:00,  6.74it/s]\n"
     ]
    }
   ],
   "source": [
    "mapping = {'N':0,'P':1}\n",
    "\n",
    "def data_prep(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for element in tqdm(dataset):\n",
    "        X.append(feature_extraction(element['sentence']))\n",
    "        y_val = np.zeros(2)\n",
    "        y_val[mapping[element['value']]] = 1\n",
    "        y.append(y_val)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = data_prep(train)\n",
    "X_val, y_val = data_prep(val)\n",
    "X_test, y_test = data_prep(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 925)               711325    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 925)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               118528    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 834,047\n",
      "Trainable params: 834,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class_counts = [0, 0]\n",
    "for el in y_train:\n",
    "    class_counts[np.argmax(el)]+=1\n",
    "class_weights = {idx:sum(class_counts)/el for idx, el in enumerate(class_counts)}\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(925, activation='tanh', input_shape=(768,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adagrad(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.6375 - accuracy: 0.5207 - val_loss: 0.6604 - val_accuracy: 0.6058\n",
      "Epoch 2/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.5600 - accuracy: 0.5402 - val_loss: 0.6503 - val_accuracy: 0.6186\n",
      "Epoch 3/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.5008 - accuracy: 0.5564 - val_loss: 0.6432 - val_accuracy: 0.6269\n",
      "Epoch 4/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.4618 - accuracy: 0.5647 - val_loss: 0.6374 - val_accuracy: 0.6305\n",
      "Epoch 5/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.4367 - accuracy: 0.5698 - val_loss: 0.6332 - val_accuracy: 0.6391\n",
      "Epoch 6/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.4280 - accuracy: 0.5720 - val_loss: 0.6305 - val_accuracy: 0.6451\n",
      "Epoch 7/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.4084 - accuracy: 0.5795 - val_loss: 0.6284 - val_accuracy: 0.6467\n",
      "Epoch 8/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3976 - accuracy: 0.5836 - val_loss: 0.6265 - val_accuracy: 0.6505\n",
      "Epoch 9/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3890 - accuracy: 0.5853 - val_loss: 0.6270 - val_accuracy: 0.6471\n",
      "Epoch 10/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3703 - accuracy: 0.5931 - val_loss: 0.6248 - val_accuracy: 0.6512\n",
      "Epoch 11/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3686 - accuracy: 0.5962 - val_loss: 0.6225 - val_accuracy: 0.6550\n",
      "Epoch 12/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3585 - accuracy: 0.6004 - val_loss: 0.6213 - val_accuracy: 0.6557\n",
      "Epoch 13/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3437 - accuracy: 0.6094 - val_loss: 0.6215 - val_accuracy: 0.6577\n",
      "Epoch 14/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3394 - accuracy: 0.6085 - val_loss: 0.6204 - val_accuracy: 0.6590\n",
      "Epoch 15/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3332 - accuracy: 0.6102 - val_loss: 0.6199 - val_accuracy: 0.6561\n",
      "Epoch 16/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3248 - accuracy: 0.6140 - val_loss: 0.6190 - val_accuracy: 0.6579\n",
      "Epoch 17/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3294 - accuracy: 0.6120 - val_loss: 0.6184 - val_accuracy: 0.6575\n",
      "Epoch 18/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3204 - accuracy: 0.6180 - val_loss: 0.6192 - val_accuracy: 0.6579\n",
      "Epoch 19/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3163 - accuracy: 0.6212 - val_loss: 0.6180 - val_accuracy: 0.6579\n",
      "Epoch 20/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3148 - accuracy: 0.6216 - val_loss: 0.6192 - val_accuracy: 0.6563\n",
      "Epoch 21/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3099 - accuracy: 0.6238 - val_loss: 0.6173 - val_accuracy: 0.6604\n",
      "Epoch 22/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3058 - accuracy: 0.6269 - val_loss: 0.6176 - val_accuracy: 0.6602\n",
      "Epoch 23/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.3003 - accuracy: 0.6265 - val_loss: 0.6173 - val_accuracy: 0.6622\n",
      "Epoch 24/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2982 - accuracy: 0.6306 - val_loss: 0.6165 - val_accuracy: 0.6584\n",
      "Epoch 25/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2992 - accuracy: 0.6284 - val_loss: 0.6162 - val_accuracy: 0.6599\n",
      "Epoch 26/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2900 - accuracy: 0.6323 - val_loss: 0.6163 - val_accuracy: 0.6626\n",
      "Epoch 27/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2961 - accuracy: 0.6265 - val_loss: 0.6159 - val_accuracy: 0.6613\n",
      "Epoch 28/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2928 - accuracy: 0.6311 - val_loss: 0.6158 - val_accuracy: 0.6606\n",
      "Epoch 29/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2903 - accuracy: 0.6359 - val_loss: 0.6156 - val_accuracy: 0.6602\n",
      "Epoch 30/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2866 - accuracy: 0.6390 - val_loss: 0.6154 - val_accuracy: 0.6599\n",
      "Epoch 31/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2809 - accuracy: 0.6368 - val_loss: 0.6153 - val_accuracy: 0.6608\n",
      "Epoch 32/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2822 - accuracy: 0.6365 - val_loss: 0.6152 - val_accuracy: 0.6624\n",
      "Epoch 33/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2847 - accuracy: 0.6374 - val_loss: 0.6153 - val_accuracy: 0.6642\n",
      "Epoch 34/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2830 - accuracy: 0.6378 - val_loss: 0.6149 - val_accuracy: 0.6640\n",
      "Epoch 35/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2812 - accuracy: 0.6430 - val_loss: 0.6147 - val_accuracy: 0.6642\n",
      "Epoch 36/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2824 - accuracy: 0.6380 - val_loss: 0.6146 - val_accuracy: 0.6624\n",
      "Epoch 37/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2813 - accuracy: 0.6391 - val_loss: 0.6147 - val_accuracy: 0.6633\n",
      "Epoch 38/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2842 - accuracy: 0.6389 - val_loss: 0.6148 - val_accuracy: 0.6635\n",
      "Epoch 39/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2771 - accuracy: 0.6415 - val_loss: 0.6149 - val_accuracy: 0.6575\n",
      "Epoch 40/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2759 - accuracy: 0.6388 - val_loss: 0.6150 - val_accuracy: 0.6635\n",
      "Epoch 41/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2771 - accuracy: 0.6407 - val_loss: 0.6146 - val_accuracy: 0.6649\n",
      "Epoch 42/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2728 - accuracy: 0.6416 - val_loss: 0.6142 - val_accuracy: 0.6660\n",
      "Epoch 43/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2746 - accuracy: 0.6426 - val_loss: 0.6146 - val_accuracy: 0.6590\n",
      "Epoch 44/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2714 - accuracy: 0.6403 - val_loss: 0.6159 - val_accuracy: 0.6536\n",
      "Epoch 45/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2756 - accuracy: 0.6432 - val_loss: 0.6138 - val_accuracy: 0.6649\n",
      "Epoch 46/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2776 - accuracy: 0.6429 - val_loss: 0.6138 - val_accuracy: 0.6660\n",
      "Epoch 47/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2700 - accuracy: 0.6445 - val_loss: 0.6141 - val_accuracy: 0.6673\n",
      "Epoch 48/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2752 - accuracy: 0.6416 - val_loss: 0.6142 - val_accuracy: 0.6676\n",
      "Epoch 49/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2737 - accuracy: 0.6464 - val_loss: 0.6137 - val_accuracy: 0.6664\n",
      "Epoch 50/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2663 - accuracy: 0.6459 - val_loss: 0.6136 - val_accuracy: 0.6664\n",
      "Epoch 51/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2708 - accuracy: 0.6453 - val_loss: 0.6137 - val_accuracy: 0.6676\n",
      "Epoch 52/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2675 - accuracy: 0.6451 - val_loss: 0.6135 - val_accuracy: 0.6644\n",
      "Epoch 53/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2689 - accuracy: 0.6454 - val_loss: 0.6135 - val_accuracy: 0.6649\n",
      "Epoch 54/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2695 - accuracy: 0.6413 - val_loss: 0.6135 - val_accuracy: 0.6667\n",
      "Epoch 55/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2688 - accuracy: 0.6457 - val_loss: 0.6134 - val_accuracy: 0.6682\n",
      "Epoch 56/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2722 - accuracy: 0.6427 - val_loss: 0.6133 - val_accuracy: 0.6660\n",
      "Epoch 57/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2689 - accuracy: 0.6461 - val_loss: 0.6132 - val_accuracy: 0.6662\n",
      "Epoch 58/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2653 - accuracy: 0.6504 - val_loss: 0.6137 - val_accuracy: 0.6597\n",
      "Epoch 59/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2612 - accuracy: 0.6483 - val_loss: 0.6131 - val_accuracy: 0.6655\n",
      "Epoch 60/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2642 - accuracy: 0.6479 - val_loss: 0.6147 - val_accuracy: 0.6521\n",
      "Epoch 61/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2596 - accuracy: 0.6538 - val_loss: 0.6138 - val_accuracy: 0.6669\n",
      "Epoch 62/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2610 - accuracy: 0.6510 - val_loss: 0.6132 - val_accuracy: 0.6606\n",
      "Epoch 63/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2694 - accuracy: 0.6496 - val_loss: 0.6129 - val_accuracy: 0.6664\n",
      "Epoch 64/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2658 - accuracy: 0.6486 - val_loss: 0.6147 - val_accuracy: 0.6525\n",
      "Epoch 65/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2591 - accuracy: 0.6474 - val_loss: 0.6130 - val_accuracy: 0.6662\n",
      "Epoch 66/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2627 - accuracy: 0.6469 - val_loss: 0.6134 - val_accuracy: 0.6682\n",
      "Epoch 67/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2637 - accuracy: 0.6466 - val_loss: 0.6130 - val_accuracy: 0.6691\n",
      "Epoch 68/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2608 - accuracy: 0.6529 - val_loss: 0.6131 - val_accuracy: 0.6689\n",
      "Epoch 69/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2625 - accuracy: 0.6507 - val_loss: 0.6130 - val_accuracy: 0.6694\n",
      "Epoch 70/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2640 - accuracy: 0.6489 - val_loss: 0.6129 - val_accuracy: 0.6662\n",
      "Epoch 71/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2598 - accuracy: 0.6536 - val_loss: 0.6132 - val_accuracy: 0.6685\n",
      "Epoch 72/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2572 - accuracy: 0.6523 - val_loss: 0.6127 - val_accuracy: 0.6691\n",
      "Epoch 73/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2591 - accuracy: 0.6511 - val_loss: 0.6128 - val_accuracy: 0.6705\n",
      "Epoch 74/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2509 - accuracy: 0.6549 - val_loss: 0.6126 - val_accuracy: 0.6617\n",
      "Epoch 75/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2562 - accuracy: 0.6531 - val_loss: 0.6128 - val_accuracy: 0.6608\n",
      "Epoch 76/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2583 - accuracy: 0.6489 - val_loss: 0.6139 - val_accuracy: 0.6543\n",
      "Epoch 77/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2591 - accuracy: 0.6530 - val_loss: 0.6126 - val_accuracy: 0.6629\n",
      "Epoch 78/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2579 - accuracy: 0.6565 - val_loss: 0.6130 - val_accuracy: 0.6590\n",
      "Epoch 79/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2543 - accuracy: 0.6553 - val_loss: 0.6125 - val_accuracy: 0.6629\n",
      "Epoch 80/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2541 - accuracy: 0.6539 - val_loss: 0.6125 - val_accuracy: 0.6646\n",
      "Epoch 81/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2544 - accuracy: 0.6549 - val_loss: 0.6125 - val_accuracy: 0.6653\n",
      "Epoch 82/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2609 - accuracy: 0.6522 - val_loss: 0.6128 - val_accuracy: 0.6691\n",
      "Epoch 83/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2536 - accuracy: 0.6553 - val_loss: 0.6125 - val_accuracy: 0.6631\n",
      "Epoch 84/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2531 - accuracy: 0.6516 - val_loss: 0.6124 - val_accuracy: 0.6629\n",
      "Epoch 85/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2464 - accuracy: 0.6571 - val_loss: 0.6125 - val_accuracy: 0.6696\n",
      "Epoch 86/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2521 - accuracy: 0.6537 - val_loss: 0.6124 - val_accuracy: 0.6635\n",
      "Epoch 87/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2551 - accuracy: 0.6506 - val_loss: 0.6126 - val_accuracy: 0.6680\n",
      "Epoch 88/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2530 - accuracy: 0.6558 - val_loss: 0.6123 - val_accuracy: 0.6698\n",
      "Epoch 89/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2524 - accuracy: 0.6542 - val_loss: 0.6122 - val_accuracy: 0.6673\n",
      "Epoch 90/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2477 - accuracy: 0.6585 - val_loss: 0.6127 - val_accuracy: 0.6703\n",
      "Epoch 91/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2530 - accuracy: 0.6555 - val_loss: 0.6122 - val_accuracy: 0.6635\n",
      "Epoch 92/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2571 - accuracy: 0.6551 - val_loss: 0.6122 - val_accuracy: 0.6633\n",
      "Epoch 93/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2534 - accuracy: 0.6558 - val_loss: 0.6125 - val_accuracy: 0.6602\n",
      "Epoch 94/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2516 - accuracy: 0.6560 - val_loss: 0.6121 - val_accuracy: 0.6658\n",
      "Epoch 95/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2521 - accuracy: 0.6562 - val_loss: 0.6123 - val_accuracy: 0.6703\n",
      "Epoch 96/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2468 - accuracy: 0.6567 - val_loss: 0.6122 - val_accuracy: 0.6644\n",
      "Epoch 97/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2568 - accuracy: 0.6537 - val_loss: 0.6124 - val_accuracy: 0.6696\n",
      "Epoch 98/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2501 - accuracy: 0.6581 - val_loss: 0.6121 - val_accuracy: 0.6664\n",
      "Epoch 99/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2495 - accuracy: 0.6581 - val_loss: 0.6122 - val_accuracy: 0.6680\n",
      "Epoch 100/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2487 - accuracy: 0.6508 - val_loss: 0.6121 - val_accuracy: 0.6658\n",
      "Epoch 101/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2402 - accuracy: 0.6593 - val_loss: 0.6122 - val_accuracy: 0.6651\n",
      "Epoch 102/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2489 - accuracy: 0.6553 - val_loss: 0.6123 - val_accuracy: 0.6689\n",
      "Epoch 103/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2491 - accuracy: 0.6572 - val_loss: 0.6122 - val_accuracy: 0.6682\n",
      "Epoch 104/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2507 - accuracy: 0.6552 - val_loss: 0.6123 - val_accuracy: 0.6622\n",
      "Epoch 105/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2457 - accuracy: 0.6606 - val_loss: 0.6121 - val_accuracy: 0.6629\n",
      "Epoch 106/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2482 - accuracy: 0.6559 - val_loss: 0.6121 - val_accuracy: 0.6633\n",
      "Epoch 107/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2466 - accuracy: 0.6561 - val_loss: 0.6120 - val_accuracy: 0.6655\n",
      "Epoch 108/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2441 - accuracy: 0.6563 - val_loss: 0.6121 - val_accuracy: 0.6660\n",
      "Epoch 109/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2422 - accuracy: 0.6617 - val_loss: 0.6121 - val_accuracy: 0.6689\n",
      "Epoch 110/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2483 - accuracy: 0.6609 - val_loss: 0.6121 - val_accuracy: 0.6671\n",
      "Epoch 111/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2462 - accuracy: 0.6611 - val_loss: 0.6121 - val_accuracy: 0.6655\n",
      "Epoch 112/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2466 - accuracy: 0.6560 - val_loss: 0.6122 - val_accuracy: 0.6698\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2432 - accuracy: 0.6573 - val_loss: 0.6124 - val_accuracy: 0.6629\n",
      "Epoch 114/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2463 - accuracy: 0.6551 - val_loss: 0.6120 - val_accuracy: 0.6655\n",
      "Epoch 115/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2463 - accuracy: 0.6566 - val_loss: 0.6119 - val_accuracy: 0.6646\n",
      "Epoch 116/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2485 - accuracy: 0.6580 - val_loss: 0.6119 - val_accuracy: 0.6651\n",
      "Epoch 117/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2426 - accuracy: 0.6605 - val_loss: 0.6120 - val_accuracy: 0.6662\n",
      "Epoch 118/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2422 - accuracy: 0.6610 - val_loss: 0.6119 - val_accuracy: 0.6651\n",
      "Epoch 119/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2437 - accuracy: 0.6584 - val_loss: 0.6119 - val_accuracy: 0.6655\n",
      "Epoch 120/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2440 - accuracy: 0.6604 - val_loss: 0.6119 - val_accuracy: 0.6671\n",
      "Epoch 121/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2441 - accuracy: 0.6583 - val_loss: 0.6118 - val_accuracy: 0.6651\n",
      "Epoch 122/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2442 - accuracy: 0.6596 - val_loss: 0.6118 - val_accuracy: 0.6642\n",
      "Epoch 123/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2465 - accuracy: 0.6592 - val_loss: 0.6122 - val_accuracy: 0.6687\n",
      "Epoch 124/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2434 - accuracy: 0.6624 - val_loss: 0.6120 - val_accuracy: 0.6653\n",
      "Epoch 125/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2390 - accuracy: 0.6633 - val_loss: 0.6120 - val_accuracy: 0.6696\n",
      "Epoch 126/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2414 - accuracy: 0.6599 - val_loss: 0.6119 - val_accuracy: 0.6696\n",
      "Epoch 127/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2406 - accuracy: 0.6631 - val_loss: 0.6117 - val_accuracy: 0.6653\n",
      "Epoch 128/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2433 - accuracy: 0.6588 - val_loss: 0.6117 - val_accuracy: 0.6655\n",
      "Epoch 129/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2425 - accuracy: 0.6588 - val_loss: 0.6117 - val_accuracy: 0.6669\n",
      "Epoch 130/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2402 - accuracy: 0.6604 - val_loss: 0.6124 - val_accuracy: 0.6714\n",
      "Epoch 131/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2424 - accuracy: 0.6621 - val_loss: 0.6121 - val_accuracy: 0.6644\n",
      "Epoch 132/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2384 - accuracy: 0.6586 - val_loss: 0.6121 - val_accuracy: 0.6644\n",
      "Epoch 133/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2372 - accuracy: 0.6616 - val_loss: 0.6125 - val_accuracy: 0.6593\n",
      "Epoch 134/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2409 - accuracy: 0.6628 - val_loss: 0.6118 - val_accuracy: 0.6682\n",
      "Epoch 135/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2446 - accuracy: 0.6600 - val_loss: 0.6119 - val_accuracy: 0.6687\n",
      "Epoch 136/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2430 - accuracy: 0.6609 - val_loss: 0.6118 - val_accuracy: 0.6669\n",
      "Epoch 137/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2422 - accuracy: 0.6626 - val_loss: 0.6118 - val_accuracy: 0.6644\n",
      "Epoch 138/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2387 - accuracy: 0.6640 - val_loss: 0.6117 - val_accuracy: 0.6655\n",
      "Epoch 139/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2485 - accuracy: 0.6568 - val_loss: 0.6117 - val_accuracy: 0.6673\n",
      "Epoch 140/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2417 - accuracy: 0.6612 - val_loss: 0.6123 - val_accuracy: 0.6694\n",
      "Epoch 141/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2398 - accuracy: 0.6619 - val_loss: 0.6117 - val_accuracy: 0.6685\n",
      "Epoch 142/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2374 - accuracy: 0.6633 - val_loss: 0.6116 - val_accuracy: 0.6669\n",
      "Epoch 143/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2372 - accuracy: 0.6636 - val_loss: 0.6117 - val_accuracy: 0.6671\n",
      "Epoch 144/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2398 - accuracy: 0.6644 - val_loss: 0.6117 - val_accuracy: 0.6685\n",
      "Epoch 145/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2376 - accuracy: 0.6603 - val_loss: 0.6118 - val_accuracy: 0.6682\n",
      "Epoch 146/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2358 - accuracy: 0.6620 - val_loss: 0.6117 - val_accuracy: 0.6685\n",
      "Epoch 147/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2422 - accuracy: 0.6610 - val_loss: 0.6116 - val_accuracy: 0.6664\n",
      "Epoch 148/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2413 - accuracy: 0.6626 - val_loss: 0.6118 - val_accuracy: 0.6671\n",
      "Epoch 149/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2383 - accuracy: 0.6660 - val_loss: 0.6117 - val_accuracy: 0.6658\n",
      "Epoch 150/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2397 - accuracy: 0.6628 - val_loss: 0.6116 - val_accuracy: 0.6660\n",
      "Epoch 151/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2420 - accuracy: 0.6609 - val_loss: 0.6119 - val_accuracy: 0.6671\n",
      "Epoch 152/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2344 - accuracy: 0.6618 - val_loss: 0.6116 - val_accuracy: 0.6660\n",
      "Epoch 153/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2411 - accuracy: 0.6644 - val_loss: 0.6119 - val_accuracy: 0.6676- loss: 1.237\n",
      "Epoch 154/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2434 - accuracy: 0.6593 - val_loss: 0.6115 - val_accuracy: 0.6678\n",
      "Epoch 155/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2406 - accuracy: 0.6632 - val_loss: 0.6115 - val_accuracy: 0.6680\n",
      "Epoch 156/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2374 - accuracy: 0.6626 - val_loss: 0.6115 - val_accuracy: 0.6669\n",
      "Epoch 157/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2384 - accuracy: 0.6613 - val_loss: 0.6116 - val_accuracy: 0.6671\n",
      "Epoch 158/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2370 - accuracy: 0.6612 - val_loss: 0.6115 - val_accuracy: 0.6676\n",
      "Epoch 159/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2326 - accuracy: 0.6653 - val_loss: 0.6118 - val_accuracy: 0.6680\n",
      "Epoch 160/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2381 - accuracy: 0.6650 - val_loss: 0.6117 - val_accuracy: 0.6680\n",
      "Epoch 161/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2365 - accuracy: 0.6616 - val_loss: 0.6122 - val_accuracy: 0.6680\n",
      "Epoch 162/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2348 - accuracy: 0.6643 - val_loss: 0.6125 - val_accuracy: 0.6602\n",
      "Epoch 163/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2356 - accuracy: 0.6635 - val_loss: 0.6116 - val_accuracy: 0.6667\n",
      "Epoch 164/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2342 - accuracy: 0.6657 - val_loss: 0.6119 - val_accuracy: 0.6649\n",
      "Epoch 165/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2363 - accuracy: 0.6656 - val_loss: 0.6116 - val_accuracy: 0.6669\n",
      "Epoch 166/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2322 - accuracy: 0.6634 - val_loss: 0.6125 - val_accuracy: 0.6602\n",
      "Epoch 167/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2347 - accuracy: 0.6626 - val_loss: 0.6117 - val_accuracy: 0.6678\n",
      "Epoch 168/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2357 - accuracy: 0.6655 - val_loss: 0.6118 - val_accuracy: 0.6689\n",
      "Epoch 169/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2383 - accuracy: 0.6610 - val_loss: 0.6115 - val_accuracy: 0.6676\n",
      "Epoch 170/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2365 - accuracy: 0.6620 - val_loss: 0.6115 - val_accuracy: 0.6678\n",
      "Epoch 171/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2397 - accuracy: 0.6627 - val_loss: 0.6116 - val_accuracy: 0.6671\n",
      "Epoch 172/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2400 - accuracy: 0.6599 - val_loss: 0.6114 - val_accuracy: 0.6680\n",
      "Epoch 173/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2345 - accuracy: 0.6631 - val_loss: 0.6114 - val_accuracy: 0.6682\n",
      "Epoch 174/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2347 - accuracy: 0.6640 - val_loss: 0.6114 - val_accuracy: 0.6660\n",
      "Epoch 175/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2358 - accuracy: 0.6631 - val_loss: 0.6114 - val_accuracy: 0.6682\n",
      "Epoch 176/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2366 - accuracy: 0.6603 - val_loss: 0.6114 - val_accuracy: 0.6685\n",
      "Epoch 177/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2315 - accuracy: 0.6671 - val_loss: 0.6116 - val_accuracy: 0.6662\n",
      "Epoch 178/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2369 - accuracy: 0.6651 - val_loss: 0.6116 - val_accuracy: 0.6671\n",
      "Epoch 179/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2308 - accuracy: 0.6682 - val_loss: 0.6114 - val_accuracy: 0.6678\n",
      "Epoch 180/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2390 - accuracy: 0.6618 - val_loss: 0.6114 - val_accuracy: 0.6678\n",
      "Epoch 181/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2348 - accuracy: 0.6652 - val_loss: 0.6114 - val_accuracy: 0.6689\n",
      "Epoch 182/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2332 - accuracy: 0.6615 - val_loss: 0.6115 - val_accuracy: 0.6680\n",
      "Epoch 183/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2363 - accuracy: 0.6628 - val_loss: 0.6114 - val_accuracy: 0.6687\n",
      "Epoch 184/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2336 - accuracy: 0.6672 - val_loss: 0.6115 - val_accuracy: 0.6667\n",
      "Epoch 185/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2293 - accuracy: 0.6673 - val_loss: 0.6114 - val_accuracy: 0.6671\n",
      "Epoch 186/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2296 - accuracy: 0.6651 - val_loss: 0.6117 - val_accuracy: 0.6691\n",
      "Epoch 187/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2354 - accuracy: 0.6647 - val_loss: 0.6116 - val_accuracy: 0.6644\n",
      "Epoch 188/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2326 - accuracy: 0.6669 - val_loss: 0.6115 - val_accuracy: 0.6671\n",
      "Epoch 189/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2326 - accuracy: 0.6627 - val_loss: 0.6115 - val_accuracy: 0.6671\n",
      "Epoch 190/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2307 - accuracy: 0.6666 - val_loss: 0.6114 - val_accuracy: 0.6680\n",
      "Epoch 191/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2326 - accuracy: 0.6632 - val_loss: 0.6117 - val_accuracy: 0.6687\n",
      "Epoch 192/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2331 - accuracy: 0.6647 - val_loss: 0.6119 - val_accuracy: 0.6678\n",
      "Epoch 193/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2292 - accuracy: 0.6656 - val_loss: 0.6116 - val_accuracy: 0.6676\n",
      "Epoch 194/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2300 - accuracy: 0.6663 - val_loss: 0.6115 - val_accuracy: 0.6655\n",
      "Epoch 195/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2305 - accuracy: 0.6642 - val_loss: 0.6114 - val_accuracy: 0.6676\n",
      "Epoch 196/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2287 - accuracy: 0.6664 - val_loss: 0.6118 - val_accuracy: 0.6667\n",
      "Epoch 197/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2316 - accuracy: 0.6697 - val_loss: 0.6113 - val_accuracy: 0.6678\n",
      "Epoch 198/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2317 - accuracy: 0.6672 - val_loss: 0.6119 - val_accuracy: 0.6680\n",
      "Epoch 199/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2268 - accuracy: 0.6709 - val_loss: 0.6114 - val_accuracy: 0.6671\n",
      "Epoch 200/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2309 - accuracy: 0.6687 - val_loss: 0.6125 - val_accuracy: 0.6586\n",
      "Epoch 201/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2334 - accuracy: 0.6653 - val_loss: 0.6114 - val_accuracy: 0.6669\n",
      "Epoch 202/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2308 - accuracy: 0.6650 - val_loss: 0.6114 - val_accuracy: 0.6694\n",
      "Epoch 203/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2286 - accuracy: 0.6651 - val_loss: 0.6114 - val_accuracy: 0.6660\n",
      "Epoch 204/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2284 - accuracy: 0.6666 - val_loss: 0.6114 - val_accuracy: 0.6669\n",
      "Epoch 205/500\n",
      "279/279 [==============================] - 3s 12ms/step - loss: 1.2317 - accuracy: 0.6680 - val_loss: 0.6118 - val_accuracy: 0.6671\n",
      "Epoch 206/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2269 - accuracy: 0.6705 - val_loss: 0.6116 - val_accuracy: 0.6673\n",
      "Epoch 207/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2304 - accuracy: 0.6634 - val_loss: 0.6115 - val_accuracy: 0.6673\n",
      "Epoch 208/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2254 - accuracy: 0.6679 - val_loss: 0.6113 - val_accuracy: 0.6678\n",
      "Epoch 209/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2316 - accuracy: 0.6647 - val_loss: 0.6116 - val_accuracy: 0.6673\n",
      "Epoch 210/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2295 - accuracy: 0.6672 - val_loss: 0.6113 - val_accuracy: 0.6680\n",
      "Epoch 211/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2300 - accuracy: 0.6679 - val_loss: 0.6123 - val_accuracy: 0.6671\n",
      "Epoch 212/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2312 - accuracy: 0.6646 - val_loss: 0.6114 - val_accuracy: 0.6682\n",
      "Epoch 213/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2281 - accuracy: 0.6649 - val_loss: 0.6114 - val_accuracy: 0.6682\n",
      "Epoch 214/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2258 - accuracy: 0.6668 - val_loss: 0.6117 - val_accuracy: 0.6635\n",
      "Epoch 215/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2317 - accuracy: 0.6641 - val_loss: 0.6117 - val_accuracy: 0.6669\n",
      "Epoch 216/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2301 - accuracy: 0.6678 - val_loss: 0.6114 - val_accuracy: 0.6667\n",
      "Epoch 217/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2335 - accuracy: 0.6666 - val_loss: 0.6114 - val_accuracy: 0.6676\n",
      "Epoch 218/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2277 - accuracy: 0.6704 - val_loss: 0.6116 - val_accuracy: 0.6680\n",
      "Epoch 219/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2249 - accuracy: 0.6714 - val_loss: 0.6115 - val_accuracy: 0.6637\n",
      "Epoch 220/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2327 - accuracy: 0.6625 - val_loss: 0.6118 - val_accuracy: 0.6667\n",
      "Epoch 221/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2277 - accuracy: 0.6661 - val_loss: 0.6114 - val_accuracy: 0.6680\n",
      "Epoch 222/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2278 - accuracy: 0.6684 - val_loss: 0.6113 - val_accuracy: 0.6682\n",
      "Epoch 223/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2268 - accuracy: 0.6688 - val_loss: 0.6114 - val_accuracy: 0.6676\n",
      "Epoch 224/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2253 - accuracy: 0.6663 - val_loss: 0.6114 - val_accuracy: 0.6682\n",
      "Epoch 225/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2299 - accuracy: 0.6667 - val_loss: 0.6116 - val_accuracy: 0.6671\n",
      "Epoch 226/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2283 - accuracy: 0.6668 - val_loss: 0.6114 - val_accuracy: 0.6685\n",
      "Epoch 227/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2256 - accuracy: 0.6675 - val_loss: 0.6114 - val_accuracy: 0.6671\n",
      "Epoch 228/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2231 - accuracy: 0.6704 - val_loss: 0.6116 - val_accuracy: 0.6640\n",
      "Epoch 229/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2236 - accuracy: 0.6702 - val_loss: 0.6115 - val_accuracy: 0.6658\n",
      "Epoch 230/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2277 - accuracy: 0.6669 - val_loss: 0.6115 - val_accuracy: 0.6651\n",
      "Epoch 231/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2296 - accuracy: 0.6671 - val_loss: 0.6120 - val_accuracy: 0.6629\n",
      "Epoch 232/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2294 - accuracy: 0.6645 - val_loss: 0.6114 - val_accuracy: 0.6671\n",
      "Epoch 233/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2264 - accuracy: 0.6679 - val_loss: 0.6115 - val_accuracy: 0.6682\n",
      "Epoch 234/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2291 - accuracy: 0.6667 - val_loss: 0.6115 - val_accuracy: 0.6680\n",
      "Epoch 235/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2232 - accuracy: 0.6677 - val_loss: 0.6117 - val_accuracy: 0.6633\n",
      "Epoch 236/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2258 - accuracy: 0.6700 - val_loss: 0.6114 - val_accuracy: 0.6673\n",
      "Epoch 237/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2284 - accuracy: 0.6668 - val_loss: 0.6123 - val_accuracy: 0.6602\n",
      "Epoch 238/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2231 - accuracy: 0.6680 - val_loss: 0.6114 - val_accuracy: 0.6669\n",
      "Epoch 239/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2334 - accuracy: 0.6669 - val_loss: 0.6113 - val_accuracy: 0.6669\n",
      "Epoch 240/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2261 - accuracy: 0.6704 - val_loss: 0.6114 - val_accuracy: 0.6649\n",
      "Epoch 241/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2237 - accuracy: 0.6688 - val_loss: 0.6114 - val_accuracy: 0.6676\n",
      "Epoch 242/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2268 - accuracy: 0.6649 - val_loss: 0.6119 - val_accuracy: 0.6629\n",
      "Epoch 243/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2255 - accuracy: 0.6700 - val_loss: 0.6114 - val_accuracy: 0.6673\n",
      "Epoch 244/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2284 - accuracy: 0.6656 - val_loss: 0.6114 - val_accuracy: 0.6673\n",
      "Epoch 245/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2238 - accuracy: 0.6692 - val_loss: 0.6125 - val_accuracy: 0.6678\n",
      "Epoch 246/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2264 - accuracy: 0.6655 - val_loss: 0.6116 - val_accuracy: 0.6660\n",
      "Epoch 247/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2227 - accuracy: 0.6673 - val_loss: 0.6113 - val_accuracy: 0.6680\n",
      "Epoch 248/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2232 - accuracy: 0.6720 - val_loss: 0.6114 - val_accuracy: 0.6669\n",
      "Epoch 249/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2281 - accuracy: 0.6687 - val_loss: 0.6114 - val_accuracy: 0.6664\n",
      "Epoch 250/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2250 - accuracy: 0.6668 - val_loss: 0.6113 - val_accuracy: 0.6673\n",
      "Epoch 251/500\n",
      "279/279 [==============================] - 3s 12ms/step - loss: 1.2269 - accuracy: 0.6706 - val_loss: 0.6115 - val_accuracy: 0.6664\n",
      "Epoch 252/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2256 - accuracy: 0.6690 - val_loss: 0.6113 - val_accuracy: 0.6664\n",
      "Epoch 253/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2256 - accuracy: 0.6695 - val_loss: 0.6113 - val_accuracy: 0.6664\n",
      "Epoch 254/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2243 - accuracy: 0.6678 - val_loss: 0.6113 - val_accuracy: 0.6673\n",
      "Epoch 255/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2261 - accuracy: 0.6661 - val_loss: 0.6114 - val_accuracy: 0.6671\n",
      "Epoch 256/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2252 - accuracy: 0.6696 - val_loss: 0.6113 - val_accuracy: 0.6658\n",
      "Epoch 257/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2258 - accuracy: 0.6679 - val_loss: 0.6113 - val_accuracy: 0.6660\n",
      "Epoch 258/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2241 - accuracy: 0.6712 - val_loss: 0.6113 - val_accuracy: 0.6669\n",
      "Epoch 259/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2226 - accuracy: 0.6666 - val_loss: 0.6113 - val_accuracy: 0.6660\n",
      "Epoch 260/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2275 - accuracy: 0.6738 - val_loss: 0.6114 - val_accuracy: 0.6660\n",
      "Epoch 261/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2258 - accuracy: 0.6687 - val_loss: 0.6120 - val_accuracy: 0.6667\n",
      "Epoch 262/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2285 - accuracy: 0.6720 - val_loss: 0.6115 - val_accuracy: 0.6653\n",
      "Epoch 263/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2218 - accuracy: 0.6733 - val_loss: 0.6115 - val_accuracy: 0.6660\n",
      "Epoch 264/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2247 - accuracy: 0.6666 - val_loss: 0.6114 - val_accuracy: 0.6658\n",
      "Epoch 265/500\n",
      "279/279 [==============================] - 3s 12ms/step - loss: 1.2232 - accuracy: 0.6673 - val_loss: 0.6115 - val_accuracy: 0.6640\n",
      "Epoch 266/500\n",
      "279/279 [==============================] - 3s 11ms/step - loss: 1.2270 - accuracy: 0.6687 - val_loss: 0.6113 - val_accuracy: 0.6664\n",
      "Epoch 267/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2219 - accuracy: 0.6694 - val_loss: 0.6116 - val_accuracy: 0.6658\n",
      "Epoch 268/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2263 - accuracy: 0.6689 - val_loss: 0.6113 - val_accuracy: 0.6651\n",
      "Epoch 269/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2249 - accuracy: 0.6675 - val_loss: 0.6113 - val_accuracy: 0.6649\n",
      "Epoch 270/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2219 - accuracy: 0.6687 - val_loss: 0.6115 - val_accuracy: 0.6646\n",
      "Epoch 271/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2213 - accuracy: 0.6681 - val_loss: 0.6114 - val_accuracy: 0.6676\n",
      "Epoch 272/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2243 - accuracy: 0.6692 - val_loss: 0.6114 - val_accuracy: 0.6655\n",
      "Epoch 273/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2242 - accuracy: 0.6692 - val_loss: 0.6114 - val_accuracy: 0.6669\n",
      "Epoch 274/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2233 - accuracy: 0.6683 - val_loss: 0.6115 - val_accuracy: 0.6669\n",
      "Epoch 275/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2217 - accuracy: 0.6728 - val_loss: 0.6113 - val_accuracy: 0.6669\n",
      "Epoch 276/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2241 - accuracy: 0.6692 - val_loss: 0.6118 - val_accuracy: 0.6664\n",
      "Epoch 277/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2200 - accuracy: 0.6716 - val_loss: 0.6114 - val_accuracy: 0.6664\n",
      "Epoch 278/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2191 - accuracy: 0.6715 - val_loss: 0.6113 - val_accuracy: 0.6653\n",
      "Epoch 279/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2211 - accuracy: 0.6708 - val_loss: 0.6114 - val_accuracy: 0.6664\n",
      "Epoch 280/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2246 - accuracy: 0.6710 - val_loss: 0.6114 - val_accuracy: 0.6662\n",
      "Epoch 281/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2241 - accuracy: 0.6705 - val_loss: 0.6119 - val_accuracy: 0.6633\n",
      "Epoch 282/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2235 - accuracy: 0.6741 - val_loss: 0.6113 - val_accuracy: 0.6655\n",
      "Epoch 283/500\n",
      "279/279 [==============================] - 3s 12ms/step - loss: 1.2223 - accuracy: 0.6688 - val_loss: 0.6115 - val_accuracy: 0.6651\n",
      "Epoch 284/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2191 - accuracy: 0.6707 - val_loss: 0.6113 - val_accuracy: 0.6667\n",
      "Epoch 285/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2210 - accuracy: 0.6673 - val_loss: 0.6115 - val_accuracy: 0.6658\n",
      "Epoch 286/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2194 - accuracy: 0.6707 - val_loss: 0.6113 - val_accuracy: 0.6660\n",
      "Epoch 287/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2199 - accuracy: 0.6707 - val_loss: 0.6113 - val_accuracy: 0.6658\n",
      "Epoch 288/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2230 - accuracy: 0.6734 - val_loss: 0.6115 - val_accuracy: 0.6662\n",
      "Epoch 289/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2221 - accuracy: 0.6679 - val_loss: 0.6114 - val_accuracy: 0.6662\n",
      "Epoch 290/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2238 - accuracy: 0.6716 - val_loss: 0.6118 - val_accuracy: 0.6646\n",
      "Epoch 291/500\n",
      "279/279 [==============================] - 3s 12ms/step - loss: 1.2215 - accuracy: 0.6706 - val_loss: 0.6116 - val_accuracy: 0.6662\n",
      "Epoch 292/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2214 - accuracy: 0.6684 - val_loss: 0.6114 - val_accuracy: 0.6660\n",
      "Epoch 293/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2232 - accuracy: 0.6695 - val_loss: 0.6114 - val_accuracy: 0.6658\n",
      "Epoch 294/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2215 - accuracy: 0.6707 - val_loss: 0.6114 - val_accuracy: 0.6669\n",
      "Epoch 295/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2222 - accuracy: 0.6667 - val_loss: 0.6114 - val_accuracy: 0.6664\n",
      "Epoch 296/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2205 - accuracy: 0.6689 - val_loss: 0.6116 - val_accuracy: 0.6662\n",
      "Epoch 297/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2240 - accuracy: 0.6722 - val_loss: 0.6116 - val_accuracy: 0.6662\n",
      "Epoch 298/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2235 - accuracy: 0.6724 - val_loss: 0.6114 - val_accuracy: 0.6662\n",
      "Epoch 299/500\n",
      "279/279 [==============================] - 4s 13ms/step - loss: 1.2220 - accuracy: 0.6730 - val_loss: 0.6115 - val_accuracy: 0.6655\n",
      "Epoch 300/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2182 - accuracy: 0.6728 - val_loss: 0.6117 - val_accuracy: 0.6644\n",
      "Epoch 301/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2220 - accuracy: 0.6694 - val_loss: 0.6115 - val_accuracy: 0.6662\n",
      "Epoch 302/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2189 - accuracy: 0.6733 - val_loss: 0.6114 - val_accuracy: 0.6662\n",
      "Epoch 303/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2196 - accuracy: 0.6691 - val_loss: 0.6114 - val_accuracy: 0.6658\n",
      "Epoch 304/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2169 - accuracy: 0.6746 - val_loss: 0.6114 - val_accuracy: 0.6667\n",
      "Epoch 305/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2202 - accuracy: 0.6735 - val_loss: 0.6114 - val_accuracy: 0.6662\n",
      "Epoch 306/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2199 - accuracy: 0.6733 - val_loss: 0.6114 - val_accuracy: 0.6655\n",
      "Epoch 307/500\n",
      "279/279 [==============================] - 4s 14ms/step - loss: 1.2196 - accuracy: 0.6733 - val_loss: 0.6115 - val_accuracy: 0.6667\n",
      "Epoch 308/500\n",
      "279/279 [==============================] - 4s 15ms/step - loss: 1.2253 - accuracy: 0.6693 - val_loss: 0.6114 - val_accuracy: 0.6662\n",
      "Epoch 00308: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(X_train), np.array(y_train),\n",
    "                    batch_size=64,\n",
    "                    epochs=500,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.652     0.702     0.676      4755\n",
      "           1      0.680     0.629     0.653      4791\n",
      "\n",
      "    accuracy                          0.665      9546\n",
      "   macro avg      0.666     0.665     0.665      9546\n",
      "weighted avg      0.666     0.665     0.665      9546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = np.argmax(y_test, 1), np.argmax(model.predict(X_test), 1)\n",
    "print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curva ROC - AUC del modelo:\n",
      "0.6653375446996768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print('Curva ROC - AUC del modelo:')\n",
    "print(roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
